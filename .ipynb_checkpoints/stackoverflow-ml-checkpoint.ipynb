{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict response time on Stackoverflow questions based on Bigquery stackoverflow public dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Data from bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the Google Cloud client library\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Instantiates a client\n",
    "bigquery_client = bigquery.Client(project='hackertone-216701')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust WHERE clause based on trainning volume, avg_ans_sec <= 7200 took 5min \n",
    "QUERY = \"\"\"\n",
    "    SELECT title,avg_ans_sec FROM `stackoverflow_summary.post_accepted_answers` WHERE avg_ans_sec <= 3*3600 \"\"\"\n",
    "\n",
    "# Or, querying dataset based on input words\n",
    "# QUERY = \"\"\"\n",
    "#     SELECT title,avg_ans_sec FROM `stackoverflow_summary.post_accepted_answers` WHERE lower(title) like '%javascript%' \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting query... ok.\n",
      "Job ID: 4e7a225e-6f56-452f-bc8f-c0f64cd6e7b0\n",
      "Query running...\n",
      "  Elapsed 7.04 s. Waiting...\n",
      "  Elapsed 8.19 s. Waiting...\n",
      "  Elapsed 9.33 s. Waiting...\n",
      "  Elapsed 10.45 s. Waiting...\n",
      "  Elapsed 11.59 s. Waiting...\n",
      "  Elapsed 12.74 s. Waiting...\n",
      "  Elapsed 13.87 s. Waiting...\n",
      "  Elapsed 15.0 s. Waiting...\n",
      "  Elapsed 16.12 s. Waiting...\n",
      "  Elapsed 17.25 s. Waiting...\n",
      "  Elapsed 18.39 s. Waiting...\n",
      "  Elapsed 19.5 s. Waiting...\n",
      "  Elapsed 20.62 s. Waiting...\n",
      "  Elapsed 21.76 s. Waiting...\n",
      "  Elapsed 22.91 s. Waiting...\n",
      "  Elapsed 24.05 s. Waiting...\n",
      "  Elapsed 25.18 s. Waiting...\n",
      "  Elapsed 26.32 s. Waiting...\n",
      "  Elapsed 27.46 s. Waiting...\n",
      "  Elapsed 28.58 s. Waiting...\n",
      "  Elapsed 29.74 s. Waiting...\n",
      "  Elapsed 30.88 s. Waiting...\n",
      "  Elapsed 32.02 s. Waiting...\n",
      "  Elapsed 33.16 s. Waiting...\n",
      "  Elapsed 34.33 s. Waiting...\n",
      "  Elapsed 35.46 s. Waiting...\n",
      "  Elapsed 36.58 s. Waiting...\n",
      "  Elapsed 37.72 s. Waiting...\n",
      "Query done.\n",
      "Processed: 399.0 MB Billed: 400.0 MB\n",
      "Standard price: $0.00 USD\n",
      "\n",
      "Retrieving results...\n",
      "Got 5795324 rows.\n",
      "\n",
      "Total time taken 269.71 s.\n",
      "Finished at 2018-10-01 09:27:18.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import read_gbq as gbq\n",
    "\n",
    "project_id = 'hackertone-216701'\n",
    "post_accepted_answers = gbq(QUERY,project_id=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize or Scale avg_ans_sec for performance improvement \n",
    "# If scale the avg_ans_sec, not easy to reverse values to original, if we reduce avg_ans_sec under 10 thousands prediction is not bad \n",
    "# from sklearn.preprocessing import scale\n",
    "\n",
    "# post_accepted_answers['avg_ans_sec_norm'] = scale(post_accepted_answers.avg_ans_sec.values)\n",
    "# post_accepted_answers.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Training and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train and test set \n",
    "X_train , X_test, y_train, y_test = train_test_split(post_accepted_answers.title, post_accepted_answers.avg_ans_sec, test_size=0.30, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize title to words\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# count_vect = CountVectorizer(lowercase=True, min_df=5)\n",
    "# X_train_counts = count_vect.fit_transform(X_train)\n",
    "# X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighting title based on TF-IDF \n",
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# tfidf_transformer = TfidfTransformer()\n",
    "# X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "# X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use Pipeline for less code \n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use Supper Vector Machines Algorithm\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Can not implement classification cause for not unique feature\n",
    "# text_clf_svm = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()), ('clf-svm', SGDClassifier(loss='hinge',penalty='l2',random_state=42))])\n",
    "\n",
    "text_reg_svm = Pipeline([('vect', CountVectorizer(stop_words='english',min_df=20)), ('tfidf', TfidfTransformer(use_idf=True)), ('reg-svm', SGDRegressor(penalty='l2',random_state=42,alpha=1e-3))])\n",
    "_ = text_reg_svm.fit(X_train, y_train)\n",
    "predicted_svm = text_reg_svm.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict diff average:1595.9731167385085\n",
      "predict diff variance:2318736.200456321\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cacluate Average Error Rate in Seconds\n",
    "print(\"predict diff average:{}\\npredict diff variance:{}\\n\".format(np.mean(abs(predicted_svm - y_test)), np.var(abs(predicted_svm - y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "#               'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "#               'vect__min_df':[5,20],\n",
    "#               'tfidf__use_idf': (True, False),\n",
    "              'reg-svm__alpha': (1e-2, 1e-3),\n",
    "             }\n",
    "\n",
    "gs_clf = GridSearchCV(text_reg_svm, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vect__min_df': 20}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_score_    \n",
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real World Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JavaScript why this mod operation return undefined [on hold]           | real response:   360.000 | predict :   835.059 | diff :   475.059 \n",
      "\n",
      "BigQuery authorization                                                 | real response: 14700.000 | predict :  1080.060 | diff :-13619.940 \n",
      "\n",
      "Replace function not working for long query in ORACLE/PLSQL?           | real response:  2400.000 | predict :   939.838 | diff : -1460.162 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Real world testing , stackoverflow queries after dataset being created.\n",
    "test_query = {'title':['JavaScript why this mod operation return undefined [on hold]',\n",
    "                       'BigQuery authorization',\n",
    "                       'Replace function not working for long query in ORACLE/PLSQL?'],\n",
    "              'response':[360,14700,2400]}\n",
    "df = pd.DataFrame(data=test_query)\n",
    "test_predict = text_reg_svm.predict(df.title)\n",
    "\n",
    "i=0\n",
    "for _,row in df.iterrows():\n",
    "  print(\"{:70s} | real response:{:10.3f} | predict :{:10.3f} | diff :{:10.3f} \\n\".format(row.title, row.response,test_predict[i],test_predict[i] - row.response))\n",
    "  i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

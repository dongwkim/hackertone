{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 빅쿼리 퍼블릭 데이터 셋을 이용해서 내 질문에 따른 Stackoverflow 응답시간 예측하기  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Data from bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the Google Cloud client library\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Instantiates a client\n",
    "bigquery_client = bigquery.Client(project='hackertone-216701')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust WHERE clause based on trainning volume, avg_ans_sec <= 7200 took 5min \n",
    "QUERY = \"\"\"\n",
    "    SELECT title,avg_ans_sec/60 as avg_ans_min FROM `stackoverflow_summary.post_accepted_answers` WHERE avg_ans_sec <= 12*3600 \"\"\"\n",
    "\n",
    "# Or, querying dataset based on input words\n",
    "# QUERY = \"\"\"\n",
    "#     SELECT title,avg_ans_sec FROM `stackoverflow_summary.post_accepted_answers` WHERE lower(title) like '%javascript%' \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting query... ok.\n",
      "Job ID: 8b6863fc-cdbd-47c7-be58-d6ffaaa25a9c\n",
      "Query running...\n",
      "  Elapsed 7.24 s. Waiting...\n",
      "  Elapsed 8.37 s. Waiting...\n",
      "  Elapsed 9.51 s. Waiting...\n",
      "  Elapsed 10.65 s. Waiting...\n",
      "  Elapsed 11.8 s. Waiting...\n",
      "  Elapsed 12.99 s. Waiting...\n",
      "  Elapsed 14.1 s. Waiting...\n",
      "  Elapsed 15.22 s. Waiting...\n",
      "  Elapsed 16.36 s. Waiting...\n",
      "  Elapsed 17.5 s. Waiting...\n",
      "  Elapsed 18.63 s. Waiting...\n",
      "  Elapsed 19.8 s. Waiting...\n",
      "  Elapsed 20.94 s. Waiting...\n",
      "  Elapsed 22.08 s. Waiting...\n",
      "  Elapsed 23.21 s. Waiting...\n",
      "  Elapsed 24.35 s. Waiting...\n",
      "  Elapsed 25.51 s. Waiting...\n",
      "  Elapsed 26.64 s. Waiting...\n",
      "  Elapsed 27.78 s. Waiting...\n",
      "  Elapsed 28.95 s. Waiting...\n",
      "  Elapsed 30.08 s. Waiting...\n",
      "  Elapsed 31.22 s. Waiting...\n",
      "  Elapsed 32.36 s. Waiting...\n",
      "  Elapsed 33.49 s. Waiting...\n",
      "  Elapsed 34.62 s. Waiting...\n",
      "  Elapsed 35.8 s. Waiting...\n",
      "  Elapsed 36.92 s. Waiting...\n",
      "  Elapsed 38.06 s. Waiting...\n",
      "  Elapsed 39.19 s. Waiting...\n",
      "  Elapsed 40.31 s. Waiting...\n",
      "  Elapsed 41.43 s. Waiting...\n",
      "  Elapsed 42.56 s. Waiting...\n",
      "  Elapsed 43.68 s. Waiting...\n",
      "  Elapsed 44.81 s. Waiting...\n",
      "  Elapsed 45.94 s. Waiting...\n",
      "  Elapsed 47.07 s. Waiting...\n",
      "  Elapsed 48.22 s. Waiting...\n",
      "  Elapsed 49.37 s. Waiting...\n",
      "  Elapsed 50.48 s. Waiting...\n",
      "  Elapsed 51.63 s. Waiting...\n",
      "  Elapsed 52.74 s. Waiting...\n",
      "  Elapsed 53.86 s. Waiting...\n",
      "  Elapsed 55.0 s. Waiting...\n",
      "  Elapsed 56.16 s. Waiting...\n",
      "  Elapsed 57.3 s. Waiting...\n",
      "  Elapsed 58.46 s. Waiting...\n",
      "  Elapsed 59.59 s. Waiting...\n",
      "  Elapsed 60.71 s. Waiting...\n",
      "  Elapsed 61.87 s. Waiting...\n",
      "  Elapsed 63.0 s. Waiting...\n",
      "  Elapsed 64.12 s. Waiting...\n",
      "  Elapsed 65.26 s. Waiting...\n",
      "  Elapsed 66.39 s. Waiting...\n",
      "  Elapsed 67.54 s. Waiting...\n",
      "  Elapsed 68.67 s. Waiting...\n",
      "  Elapsed 69.8 s. Waiting...\n",
      "  Elapsed 70.95 s. Waiting...\n",
      "  Elapsed 72.08 s. Waiting...\n",
      "  Elapsed 73.23 s. Waiting...\n",
      "  Elapsed 74.35 s. Waiting...\n",
      "  Elapsed 75.49 s. Waiting...\n",
      "  Elapsed 76.63 s. Waiting...\n",
      "  Elapsed 77.75 s. Waiting...\n",
      "  Elapsed 78.87 s. Waiting...\n",
      "  Elapsed 79.97 s. Waiting...\n",
      "  Elapsed 81.13 s. Waiting...\n",
      "  Elapsed 82.24 s. Waiting...\n",
      "  Elapsed 83.39 s. Waiting...\n",
      "  Elapsed 84.51 s. Waiting...\n",
      "Query done.\n",
      "Processed: 399.0 MB Billed: 400.0 MB\n",
      "Standard price: $0.00 USD\n",
      "\n",
      "Retrieving results...\n",
      "Got 6528396 rows.\n",
      "\n",
      "Total time taken 382.64 s.\n",
      "Finished at 2018-10-01 17:04:21.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import read_gbq as gbq\n",
    "\n",
    "project_id = 'hackertone-216701'\n",
    "post_accepted_answers = gbq(QUERY,project_id=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize or Scale avg_ans_sec for performance improvement \n",
    "# If scale the avg_ans_sec, not easy to reverse values to original, if we reduce avg_ans_sec under 10 thousands prediction is not bad \n",
    "# from sklearn.preprocessing import scale\n",
    "\n",
    "# post_accepted_answers['avg_ans_sec_norm'] = scale(post_accepted_answers.avg_ans_sec.values)\n",
    "# post_accepted_answers.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Training and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train and test set \n",
    "X_train , X_test, y_train, y_test = train_test_split(post_accepted_answers.title, post_accepted_answers.avg_ans_min, test_size=0.30, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize title to words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# count_vect = CountVectorizer(lowercase=True, min_df=5)\n",
    "# X_train_counts = count_vect.fit_transform(X_train)\n",
    "# X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighting title based on TF-IDF \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# tfidf_transformer = TfidfTransformer()\n",
    "# X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "# X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use Pipeline for less code \n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use Supper Vector Machines Algorithm\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Can not implement classification cause for not unique feature\n",
    "# text_clf_svm = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()), ('clf-svm', SGDClassifier(loss='hinge',penalty='l2',random_state=42))])\n",
    "\n",
    "text_reg_svm = Pipeline([('vect', CountVectorizer(stop_words='english',min_df=20)), ('tfidf', TfidfTransformer(use_idf=True)), ('reg-svm', SGDRegressor(penalty='l2',random_state=42,alpha=1e-3))])\n",
    "model = text_reg_svm.fit(X_train, y_train)\n",
    "predicted_svm = text_reg_svm.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict diff average:75.61794306012996\n",
      "predict diff variance:9015.556327428725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cacluate Average Error Rate in Seconds\n",
    "print(\"predict diff average:{}\\npredict diff variance:{}\\n\".format(np.mean(abs(predicted_svm - y_test)), np.var(abs(predicted_svm - y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model with Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'model.pkl'\n",
    "pickle.dump(model, open(filename,'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "#               'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "#               'vect__min_df':[5,20],\n",
    "#               'tfidf__use_idf': (True, False),\n",
    "              'reg-svm__alpha': (1e-2, 1e-3),\n",
    "             }\n",
    "\n",
    "gs_clf = GridSearchCV(text_reg_svm, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf.best_score_    \n",
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real World Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JavaScript why this mod operation return undefined [on hold]           | real response:     6.000 | predict :    51.536 | diff :    45.536 \n",
      "\n",
      "BigQuery authorization                                                 | real response:   240.000 | predict :    93.116 | diff :   146.884 \n",
      "\n",
      "Replace function not working for long query in ORACLE/PLSQL?           | real response:    40.000 | predict :    56.120 | diff :    16.120 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Real world testing , stackoverflow queries after dataset being created.\n",
    "test_query = {'title':['JavaScript why this mod operation return undefined [on hold]',\n",
    "                       'BigQuery authorization',\n",
    "                       'Replace function not working for long query in ORACLE/PLSQL?'],\n",
    "              'response':[6,240,40]}\n",
    "df = pd.DataFrame(data=test_query)\n",
    "test_predict = text_reg_svm.predict(df.title)\n",
    "\n",
    "i=0\n",
    "for _,row in df.iterrows():\n",
    "  print(\"{:70s} | real response:{:10.3f} | predict :{:10.3f} | diff :{:10.3f} \\n\".format(row.title, row.response,test_predict[i],abs(test_predict[i] - row.response)))\n",
    "  i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
